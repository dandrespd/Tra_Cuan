# ============================================
# PYTEST CONFIGURATION
# ============================================

[tool:pytest]
# Directorios de tests
testpaths = tests

# Patrones de archivos de test
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_*

# Opciones por defecto
addopts = 
    # Verbose output
    -v
    # Show locals in tracebacks
    -l
    # Strict markers
    --strict-markers
    # Warnings
    -W error::DeprecationWarning
    -W error::PendingDeprecationWarning
    # Coverage
    --cov=.
    --cov-config=.coveragerc
    --cov-report=html
    --cov-report=term-missing:skip-covered
    --cov-fail-under=80
    # Parallel execution
    -n auto
    # Show slowest tests
    --durations=10
    # Capture output
    --capture=no
    -s
    # Traceback style
    --tb=short
    # Continue on failures
    --maxfail=5

# Timeout para tests (segundos)
timeout = 300
timeout_method = thread

# Marcadores personalizados
markers =
    # Categorías de tests
    unit: Unit tests that test individual components
    integration: Integration tests that test component interactions
    e2e: End-to-end tests that test complete workflows
    slow: Tests that take more than 5 seconds
    requires_mt5: Tests that require MetaTrader 5 connection
    requires_db: Tests that require database connection
    requires_redis: Tests that require Redis connection
    requires_api: Tests that require external API access
    
    # Estrategias específicas
    ml_strategy: Tests for machine learning strategies
    technical_strategy: Tests for technical analysis strategies
    
    # Componentes específicos
    risk_management: Tests for risk management components
    execution: Tests for order execution
    data_processing: Tests for data processing pipelines
    
    # Ambientes
    live_only: Tests that should only run in live environment
    paper_only: Tests that should only run in paper trading
    ci_skip: Tests to skip in CI/CD pipeline
    
    # Performance
    benchmark: Performance benchmark tests
    stress: Stress tests for system limits
    
    # Criticidad
    critical: Critical tests that must pass
    smoke: Smoke tests for basic functionality

# Configuración de logging durante tests
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s - %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Captura de logs
log_capture = true
log_level = DEBUG

# Filtros de warnings
filterwarnings =
    # Ignorar warnings específicos
    ignore::UserWarning
    ignore::DeprecationWarning:pandas.*
    ignore::FutureWarning:numpy.*
    # Convertir en errores
    error::ResourceWarning
    error::RuntimeWarning:tradingbot.*

# Configuración de fixtures
usefixtures = 
    setup_test_env
    cleanup_after_test

# Plugins deshabilitados
disable_plugins = 
    doctest

# Configuración para pytest-xdist (parallel testing)
[pytest:xdist]
looponfail = true
numprocesses = auto

# Configuración para pytest-cov (coverage)
[coverage:run]
source = .
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */env/*
    */.venv/*
    */migrations/*
    */config/*
    setup.py
    */docs/*
    */examples/*

[coverage:report]
precision = 2
show_missing = true
skip_covered = false
exclude_lines =
    # Pragmas
    pragma: no cover
    
    # Defensive programming
    raise AssertionError
    raise NotImplementedError
    
    # Debug code
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    if DEBUG:
    if debug:
    
    # Abstract methods
    @abstract
    @abstractmethod
    
    # Property setters
    def __repr__
    def __str__
    
    # Unreachable code
    pass
    ...

[coverage:html]
directory = htmlcov

# Configuración para pytest-benchmark
[benchmark]
min_rounds = 5
min_time = 0.000005
max_time = 1.0
calibration_precision = 10
warmup = true
warmup_iterations = 100000

# Configuración específica para diferentes entornos
[testenv:unit]
commands = pytest -m "unit and not slow" {posargs}

[testenv:integration]
commands = pytest -m integration {posargs}

[testenv:e2e]
commands = pytest -m e2e {posargs}

[testenv:performance]
commands = pytest -m "benchmark or stress" {posargs}

[testenv:ci]
commands = pytest -m "not ci_skip and not requires_mt5" {posargs}